{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cPickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-92e12b206532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cPickle'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import graph\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use(\"gpu2\")\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.sparse as Tsp\n",
    "import lasagne as L\n",
    "import lasagne.init as LI\n",
    "import lasagne.layers as LL\n",
    "import lasagne.objectives as LO\n",
    "import lasagne.regularization as LR\n",
    "\n",
    "theano.config.scan.allow_gc = True\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matlab_file(path_file, name_field):\n",
    "    \"\"\"\n",
    "    load '.mat' files\n",
    "    inputs:\n",
    "        path_file, string containing the file path\n",
    "        name_field, string containig the field name (default='shape')\n",
    "    warning:\n",
    "        '.mat' files should be saved in the '-v7.3' format\n",
    "    \"\"\"\n",
    "    db = h5py.File(path_file, 'r')\n",
    "    ds = db[name_field]\n",
    "    try:\n",
    "        if 'ir' in ds.keys():\n",
    "            data = np.asarray(ds['data'])\n",
    "            ir   = np.asarray(ds['ir'])\n",
    "            jc   = np.asarray(ds['jc'])\n",
    "            out  = sp.csc_matrix((data, ir, jc)).astype(np.float32)\n",
    "    except AttributeError:\n",
    "        # Transpose in case is a dense matrix because of the row- vs column- major ordering between python and matlab\n",
    "        out = np.asarray(ds).astype(np.float32).T\n",
    "\n",
    "    db.close()\n",
    "\n",
    "    return out\n",
    "\n",
    "def stack_matrices(x, n_supPix=75): # stack 2D matrices into a 3D tensor\n",
    "    x_rho   = x[:,0:n_supPix*n_supPix]\n",
    "    x_theta = x[:, n_supPix*n_supPix:]\n",
    "    x_rho   = np.reshape(x_rho, (x_rho.shape[0], np.sqrt(x_rho.shape[1]), np.sqrt(x_rho.shape[1]), 1))\n",
    "    x_theta = np.reshape(x_theta, (x_theta.shape[0], np.sqrt(x_theta.shape[1]), np.sqrt(x_theta.shape[1]), 1))\n",
    "    \n",
    "    for k in range(x_theta.shape[0]): #set the diagonal of all the theta matrix to a value really close to 0\n",
    "        np.fill_diagonal(x_theta[k,:,:,0], 1e-14)\n",
    "    \n",
    "    y = np.concatenate([x_rho, x_theta], axis=3)\n",
    "    return y\n",
    "\n",
    "def compute_similarity_matrix(dist_matrix):\n",
    "    shp = dist_matrix.shape\n",
    "    similarity_matrix = np.zeros(shp)\n",
    "    sigma = np.mean(dist_matrix[np.isfinite(dist_matrix)])\n",
    "    for i in range(shp[0]):\n",
    "        for j in range(shp[1]):\n",
    "            if (np.isfinite(dist_matrix[i,j])):\n",
    "                dist = np.exp(-dist_matrix[i,j]**2/sigma**2) #the higher the distance the smaller the similarity\n",
    "                similarity_matrix[i, j] = dist\n",
    "            else:\n",
    "                similarity_matrix[i, j] = 0  \n",
    "    return similarity_matrix\n",
    "\n",
    "def extend_feat(feat, max_dim, value=0, patch=0):\n",
    "    max_dim = np.squeeze(max_dim)\n",
    "    feat = np.append(feat, value*np.ones((feat.shape[0], max_dim[1] - feat.shape[1])), \n",
    "                                  axis=1)\n",
    "    feat = np.append(feat, value*np.ones((max_dim[0] - feat.shape[0], feat.shape[1])), \n",
    "                                  axis=0)\n",
    "    if (patch==1):\n",
    "        feat = np.copy(feat)\n",
    "        np.fill_diagonal(feat, 0)\n",
    "    if (patch==2):\n",
    "        feat = np.copy(feat)\n",
    "        np.fill_diagonal(feat, 1e-14)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_supPix = 75\n",
    "\n",
    "# path to the main folder\n",
    "path_main     = '..'\n",
    "\n",
    "# path to the input descriptors\n",
    "path_train_vals    = os.path.join(path_main,'datasets/mnist_superpixels_data_%d/train_vals.mat' % n_supPix)\n",
    "path_test_vals    = os.path.join(path_main,'datasets/mnist_superpixels_data_%d/test_vals.mat' % n_supPix)\n",
    "\n",
    "# path to the patches\n",
    "path_coords_train  = os.path.join(path_main,'datasets/mnist_superpixels_data_%d/train_patch_coords.mat' % n_supPix)\n",
    "path_coords_test  = os.path.join(path_main,'datasets/mnist_superpixels_data_%d/test_patch_coords.mat' % n_supPix)\n",
    "\n",
    "# path to the labels\n",
    "path_train_labels   = os.path.join(path_main,'datasets/MNIST_preproc_train_labels/MNIST_labels.mat')\n",
    "path_test_labels   = os.path.join(path_main,'datasets/MNIST_preproc_test_labels/MNIST_labels.mat')\n",
    "\n",
    "# path to the idx centroids\n",
    "path_train_centroids  = os.path.join(path_main,'datasets/mnist_superpixels_data_%d/train_centroids.mat' % n_supPix)\n",
    "path_test_centroids  = os.path.join(path_main,'datasets/mnist_superpixels_data_%d/test_centroids.mat' % n_supPix)\n",
    "\n",
    "# path to the coarsening\n",
    "path_coarsening = os.path.join(path_main,'datasets/graclus_pooling_data_%d_supPix/data.dump' % n_supPix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, path_train_vals, path_test_vals, path_coords_train, path_coords_test, \n",
    "                 path_train_labels, path_test_labels, \n",
    "                 path_coarsening,\n",
    "                 idx_end_training = 60000, idx_start_training=5000, \n",
    "                 idx_start_val=0, idx_end_val=4999):\n",
    "        \n",
    "        # path to (pre-computed) descriptors\n",
    "        self.path_train_vals = path_train_vals\n",
    "        self.path_test_vals = path_test_vals\n",
    "        # path to (pre-computed) patches\n",
    "        self.path_coords_train = path_coords_train\n",
    "        self.path_coords_test = path_coords_test\n",
    "        # path to labels\n",
    "        self.path_train_labels = path_train_labels\n",
    "        self.path_test_labels = path_test_labels\n",
    "        \n",
    "        # loading the descriptors\n",
    "        print(\"[i] Loading signals\")\n",
    "        self.vals_train = load_matlab_file(self.path_train_vals, 'vals')\n",
    "        self.vals_test = load_matlab_file(self.path_test_vals, 'vals')\n",
    "        \n",
    "        self.idx_end_training = idx_end_training\n",
    "        self.idx_start_training = idx_start_training\n",
    "        self.idx_start_val = idx_start_val\n",
    "        self.idx_end_val = idx_end_val\n",
    "        \n",
    "        # loading the coords\n",
    "        print(\"[i] Loading coords\")\n",
    "        tmp = load_matlab_file(self.path_coords_train, 'patch_coords')\n",
    "        self.coords_train = stack_matrices(tmp)\n",
    "\n",
    "        tmp = load_matlab_file(self.path_coords_test, 'patch_coords')\n",
    "        self.coords_test = stack_matrices(tmp)\n",
    "            \n",
    "        #loading mapping and coarsening graphs\n",
    "        self.list_training_mapping_graph_0_to_graph_1,\\\n",
    "        self.list_training_mapping_graph_1_to_graph_2,\\\n",
    "        self.list_test_mapping_graph_0_to_graph_1,\\\n",
    "        self.list_test_mapping_graph_1_to_graph_2,\\\n",
    "        self.list_training_rho_graph_1,\\\n",
    "        self.list_training_theta_graph_1,\\\n",
    "        self.list_training_rho_graph_2,\\\n",
    "        self.list_training_theta_graph_2,\\\n",
    "        self.list_test_rho_graph_1,\\\n",
    "        self.list_test_theta_graph_1,\\\n",
    "        self.list_test_rho_graph_2,\\\n",
    "        self.list_test_theta_graph_2,\\\n",
    "        self.idx_train_centroids_original_images,\\\n",
    "        self.idx_train_centroids_graph_2,\\\n",
    "        self.idx_test_centroids_original_images,\\\n",
    "        self.idx_test_centroids_graph_2 = joblib.load(path_coarsening)\n",
    "        \n",
    "        #computation of maximum mappings dimensions\n",
    "        self.max_dim_mappings  = np.squeeze(np.asarray([[-1,-1], [-1, -1], [-1,-1]]))\n",
    "        for k in range(len(self.list_training_mapping_graph_0_to_graph_1)):\n",
    "            self.max_dim_mappings[0,0] = np.maximum(self.max_dim_mappings[0,0], \n",
    "                                                    self.list_training_mapping_graph_0_to_graph_1[k].shape[0])\n",
    "            self.max_dim_mappings[0,1] = np.maximum(self.max_dim_mappings[0,1], \n",
    "                                                    self.list_training_mapping_graph_0_to_graph_1[k].shape[1])\n",
    "            \n",
    "            self.max_dim_mappings[1,0] = np.maximum(self.max_dim_mappings[1,0], \n",
    "                                                    self.list_training_mapping_graph_1_to_graph_2[k].shape[0])\n",
    "            self.max_dim_mappings[1,1] = np.maximum(self.max_dim_mappings[1,1], \n",
    "                                                    self.list_training_mapping_graph_1_to_graph_2[k].shape[1])\n",
    "            self.max_dim_mappings[2,0] = np.maximum(self.max_dim_mappings[2,0], \n",
    "                                                    self.list_training_mapping_graph_1_to_graph_2[k].shape[0])\n",
    "            self.max_dim_mappings[2,1] = np.maximum(self.max_dim_mappings[2,1], \n",
    "                                                    self.list_training_mapping_graph_1_to_graph_2[k].shape[0])\n",
    "            \n",
    "        for k in range(len(self.list_test_mapping_graph_0_to_graph_1)):\n",
    "            self.max_dim_mappings[0,0] = np.maximum(self.max_dim_mappings[0,0], \n",
    "                                                    self.list_test_mapping_graph_0_to_graph_1[k].shape[0])\n",
    "            self.max_dim_mappings[0,1] = np.maximum(self.max_dim_mappings[0,1], \n",
    "                                                    self.list_test_mapping_graph_0_to_graph_1[k].shape[1])\n",
    "            \n",
    "            self.max_dim_mappings[1,0] = np.maximum(self.max_dim_mappings[1,0], \n",
    "                                                    self.list_test_mapping_graph_1_to_graph_2[k].shape[0])\n",
    "            self.max_dim_mappings[1,1] = np.maximum(self.max_dim_mappings[1,1], \n",
    "                                                    self.list_test_mapping_graph_1_to_graph_2[k].shape[1])\n",
    "            self.max_dim_mappings[2,0] = np.maximum(self.max_dim_mappings[2,0], \n",
    "                                                    self.list_test_mapping_graph_1_to_graph_2[k].shape[0])\n",
    "            self.max_dim_mappings[2,1] = np.maximum(self.max_dim_mappings[2,1], \n",
    "                                                    self.list_test_mapping_graph_1_to_graph_2[k].shape[0])\n",
    "        \n",
    "        \n",
    "        print(\"[i] Loading labels\")\n",
    "        self.train_labels = self.load_labels(self.path_train_labels)\n",
    "        self.test_labels = self.load_labels(self.path_test_labels)\n",
    "    \n",
    "    def load_labels(self, fname):\n",
    "        tmp = load_matlab_file(fname, 'labels')\n",
    "        tmp = tmp.astype(np.int32)\n",
    "        return tmp.flatten()\n",
    "    \n",
    "    def train_iter(self):\n",
    "        idx = np.random.permutation(self.idx_end_training - self.idx_start_training) + self.idx_start_training\n",
    "        for i in xrange(len(idx)):\n",
    "            yield (self.vals_train[idx[i],:], \n",
    "                   self.train_labels[idx[i]],\n",
    "                   self.coords_train[idx[i],:,:,0],\n",
    "                   self.coords_train[idx[i],:,:,1],\n",
    "                   self.list_training_rho_graph_1[idx[i]],\n",
    "                   self.list_training_theta_graph_1[idx[i]],\n",
    "                   self.list_training_rho_graph_2[idx[i]],\n",
    "                   self.list_training_theta_graph_2[idx[i]],\n",
    "                   self.list_training_mapping_graph_0_to_graph_1[idx[i]],\n",
    "                   self.list_training_mapping_graph_1_to_graph_2[idx[i]],\n",
    "                   self.idx_train_centroids_graph_2[idx[i]])\n",
    "     \n",
    "    def get_train_sample(self, idx):\n",
    "        return (self.vals_train[idx,:], \n",
    "                   self.train_labels[idx],\n",
    "                   self.coords_train[idx,:,:,0],\n",
    "                   self.coords_train[idx,:,:,1],\n",
    "                   self.list_training_rho_graph_1[idx],\n",
    "                   self.list_training_theta_graph_1[idx],\n",
    "                   self.list_training_rho_graph_2[idx],\n",
    "                   self.list_training_theta_graph_2[idx],\n",
    "                   self.list_training_mapping_graph_0_to_graph_1[idx],\n",
    "                   self.list_training_mapping_graph_1_to_graph_2[idx],\n",
    "                   self.idx_train_centroids_graph_2[idx])\n",
    "    \n",
    "    def get_test_sample(self, idx):\n",
    "        return (self.vals_test[idx,:], \n",
    "                   self.test_labels[idx],\n",
    "                   self.coords_test[idx,:,:,0],\n",
    "                   self.coords_test[idx,:,:,1],\n",
    "                   self.list_test_rho_graph_1[idx],\n",
    "                   self.list_test_theta_graph_1[idx],\n",
    "                   self.list_test_rho_graph_2[idx],\n",
    "                   self.list_test_theta_graph_2[idx],\n",
    "                   self.list_test_mapping_graph_0_to_graph_1[idx],\n",
    "                   self.list_test_mapping_graph_1_to_graph_2[idx],\n",
    "                   self.idx_test_centroids_graph_2[idx])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = Dataset(path_train_vals, path_test_vals, path_coords_train, path_coords_test, path_train_labels, path_test_labels, path_coarsening)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoNet Basic Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianWeightsLayer(LL.MergeLayer): #computes the membership of a neighbors to the variosu bins\n",
    "    \"\"\"\n",
    "    Fixed layer computing gaussian weights as patch operator\n",
    "    \"\"\"\n",
    "    def __init__(self, incoming, n_rho, n_theta, max_rho = 5, **kwargs):\n",
    "        super(GaussianWeightsLayer, self).__init__(incoming, **kwargs)\n",
    "        \n",
    "        # layer's attributes\n",
    "        self.n_rho       = n_rho\n",
    "        self.n_theta     = n_theta\n",
    "        self.sigma_rho   = 1.0\n",
    "        self.sigma_theta = 0.75\n",
    "        self.sigma_rho_min = 1\n",
    "        self.sigma_theta_min = 0.75\n",
    "        \n",
    "        coords = [np.random.rand(n_rho*n_theta,1) * max_rho, np.random.rand(n_rho*n_theta,1)*2*np.pi - np.pi]\n",
    "        coords = np.concatenate(coords, axis=1)\n",
    "        \n",
    "        self.coords = self.add_param(coords.astype(np.float32), coords.shape, name='coords')\n",
    "        self.sigma_rho = self.add_param((np.random.rand(coords.shape[0])*self.sigma_rho + self.sigma_rho_min).astype(np.float32), (coords.shape[0],), name='sigma_rho')\n",
    "        self.sigma_theta = self.add_param((np.random.rand(coords.shape[0])*self.sigma_theta + self.sigma_theta_min).astype(np.float32), (coords.shape[0],), name='sigma_theta')\n",
    "        self.sigma_rho = self.sigma_rho.dimshuffle('x', 'x','x', 0)\n",
    "        self.sigma_theta = self.sigma_theta.dimshuffle('x', 'x','x', 0)\n",
    "            \n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return (input_shapes[0][0], input_shapes[0][1], input_shapes[0][2], self.n_rho*self.n_theta)\n",
    "\n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "\n",
    "        P_rho, P_theta = inputs\n",
    "        \n",
    "        mu_rho   = self.coords[:,0]\n",
    "        mu_theta = self.coords[:,1]\n",
    "        mu_rho   = mu_rho.dimshuffle('x', 'x','x',0)\n",
    "        mu_theta = mu_theta.dimshuffle('x', 'x','x',0)\n",
    "        \n",
    "        P_rho   = P_rho.dimshuffle(0, 1, 2, 'x')\n",
    "        P_theta = P_theta.dimshuffle(0, 1, 2, 'x')\n",
    "\n",
    "        #computation rho weights\n",
    "        weights_rho = T.exp(-0.5*T.sqr((P_rho-mu_rho))/(1e-14+T.sqr(self.sigma_rho)))\n",
    "        \n",
    "        #computation theta weights\n",
    "        first_angle   = T.abs_(P_theta-mu_theta)#.dimshuffle(0, 1, 2, 'x')\n",
    "        second_angle  = T.abs_(2*np.pi - T.abs_(P_theta-mu_theta))#.dimshuffle(0, 1, 2, 'x')\n",
    "        weights_theta = T.exp(-0.5*T.sqr(T.minimum(first_angle, second_angle))/(1e-14+T.sqr(self.sigma_theta)))\n",
    "        \n",
    "        #computation of the final membership\n",
    "        weights = weights_rho * weights_theta \n",
    "        weights = T.switch(T.isnan(weights), 0, weights)\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyDescLayer(LL.MergeLayer): #apply the membership to the features of the various nodes\n",
    "    \n",
    "    def __init__(self, incoming, **kwargs):\n",
    "        super(ApplyDescLayer, self).__init__(incoming, **kwargs)\n",
    "        \n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return (None, input_shapes[0][3]*input_shapes[1][2] )\n",
    "\n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        weights, desc = inputs\n",
    "        \n",
    "        weights = weights.dimshuffle(0, 3, 1, 2)\n",
    "        res = T.batched_dot(weights, desc)\n",
    "        res = res.dimshuffle(0, 2, 1, 3)\n",
    "        res = T.reshape(res, (-1, res.shape[2]*res.shape[3]))\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapping(LL.MergeLayer): #max pooling\n",
    "    def __init__(self, incoming, num_orig_supPix, num_reduced_supPix, min_value = -1, **kwargs):\n",
    "        super(Mapping, self).__init__(incoming, **kwargs)\n",
    "        \n",
    "        self.num_orig_supPix = num_reduced_supPix\n",
    "        self.num_reduced_supPix = num_reduced_supPix\n",
    "        self.min_value = min_value #minimum value of the input descriptor. Put it equal to -1 for tanh and 0 for rectify\n",
    "        \n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return (None, self.num_reduced_supPix, input_shapes[0][2])\n",
    "            \n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        desc, mapping = inputs \n",
    "            \n",
    "        desc         = desc.dimshuffle(0, 'x', 1, 2)\n",
    "        mapping      = mapping.dimshuffle(0, 1, 2, 'x')\n",
    "        desc         = desc - self.min_value #every descriptor is now positive\n",
    "        final_result = mapping*desc\n",
    "        final_result = final_result.max(axis=2)\n",
    "        final_result = final_result + self.min_value\n",
    "        \n",
    "        return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_reducing_to_centroids(idx_c_img, c_img_desc, c_idx_supPix, output_desc): \n",
    "        output_desc = T.set_subtensor(output_desc[idx_c_img, :], c_img_desc[c_idx_supPix-1, :])\n",
    "        return output_desc\n",
    "\n",
    "\n",
    "class Reduce_to_centroids(LL.MergeLayer): #extracts only the features of the central node\n",
    "    def __init__(self, incoming, **kwargs):\n",
    "        super(Reduce_to_centroids, self).__init__(incoming, **kwargs)\n",
    "        \n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return (None, input_shapes[0][2])\n",
    "\n",
    "            \n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        desc, idx_centroids = inputs \n",
    "        \n",
    "        list_idx_img = T.arange(desc.shape[0])\n",
    "        result, _    = theano.scan(fn=scan_reducing_to_centroids,\n",
    "                                outputs_info=T.zeros((desc.shape[0], desc.shape[2])),\n",
    "                                sequences=[list_idx_img, desc, idx_centroids])\n",
    "\n",
    "\n",
    "        final_result = result[-1]\n",
    "        return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "n_digits = 10\n",
    "n_rho    = 5\n",
    "n_theta  = 5\n",
    "\n",
    "n_fout1  = 32\n",
    "n_fout2  = 64\n",
    "n_fout3  = 512\n",
    "\n",
    "#input variables\n",
    "P_rho_orig_img   = LL.InputLayer(shape=(None, ds.max_dim_mappings[0,1], ds.max_dim_mappings[0,1]), input_var=T.ftensor3('P_rho_orig_img'))\n",
    "P_theta_orig_img = LL.InputLayer(shape=(None, ds.max_dim_mappings[0,1], ds.max_dim_mappings[0,1]), input_var=T.ftensor3('P_theta_orig_img'))\n",
    "\n",
    "P_rho_1st_pool   = LL.InputLayer(shape=(None, ds.max_dim_mappings[0,0], ds.max_dim_mappings[0,0]), input_var=T.ftensor3('P_rho_1st_pool'))\n",
    "P_theta_1st_pool = LL.InputLayer(shape=(None, ds.max_dim_mappings[0,0], ds.max_dim_mappings[0,0]), input_var=T.ftensor3('P_theta_1st_pool'))\n",
    "P_rho_2nd_pool   = LL.InputLayer(shape=(None, ds.max_dim_mappings[1,0],  ds.max_dim_mappings[1,0]), input_var=T.ftensor3('P_rho_2nd_pool'))\n",
    "P_theta_2nd_pool = LL.InputLayer(shape=(None, ds.max_dim_mappings[1,0],  ds.max_dim_mappings[1,0]), input_var=T.ftensor3('P_theta_2nd_pool'))\n",
    "\n",
    "mappings_orig_graph_1st_pool = LL.InputLayer(shape=(None, ds.max_dim_mappings[0,0], ds.max_dim_mappings[0,1]), input_var=T.ftensor3('mappings_orig_graph_196_supPix')) #each column contain the index of the 196 supPix image where that pixel should go\n",
    "mappings_1st_pool_2nd_pool  = LL.InputLayer(shape=(None, ds.max_dim_mappings[1,0], ds.max_dim_mappings[1,1]), input_var=T.ftensor3('mappings_196_supPix_25_supPix')) #each column contain the index of the 25 supPix image where that supPix should go\n",
    "\n",
    "idx_centroids_conv3 = LL.InputLayer(shape=(None,), input_var=T.ivector('idx_centroids_conv3'))\n",
    "vals                = LL.InputLayer(shape=(None, None, 1), input_var=T.ftensor3('vals')) #dim_batch, num_pixels_img, 1\n",
    "\n",
    "#model definition\n",
    "\n",
    "#computation of the memberships. Each of the following tensors contains the membership of each neighbor \n",
    "#to the various bins for each possible vertex in the graph\n",
    "memb_orig_img = GaussianWeightsLayer([P_rho_orig_img, P_theta_orig_img], n_rho, n_theta) \n",
    "memb_1st_pool = GaussianWeightsLayer([P_rho_1st_pool, P_theta_1st_pool], n_rho, n_theta)\n",
    "memb_2nd_pool = GaussianWeightsLayer([P_rho_2nd_pool, P_theta_2nd_pool], n_rho, n_theta, max_rho = 19) #we set max_rho to a larger value for exploting the entire graph in the last conv layer\n",
    "\n",
    "# convolution layer\n",
    "net1 = ApplyDescLayer([memb_orig_img, vals]) # patch operator layer\n",
    "net1 = LL.DenseLayer(net1, n_fout1, nonlinearity=L.nonlinearities.rectify) \n",
    "net1 = LL.ReshapeLayer(net1, (-1, ds.max_dim_mappings[0,1], n_fout1))\n",
    "\n",
    "# pooling\n",
    "net1_mapped = Mapping([net1, mappings_orig_graph_1st_pool], ds.max_dim_mappings[0,1], ds.max_dim_mappings[0,0], min_value = 0) #dim_batch, 196, n_fout1\n",
    "net1_mapped = LL.DropoutLayer(net1_mapped, p=0.5)\n",
    "\n",
    "# convolution layer\n",
    "net2 = ApplyDescLayer([memb_1st_pool, net1_mapped]) # patch operator layer\n",
    "net2 = LL.DenseLayer(net2, n_fout2, nonlinearity=L.nonlinearities.rectify)\n",
    "net2 = LL.ReshapeLayer(net2, (-1, ds.max_dim_mappings[0,0], n_fout2))\n",
    "\n",
    "# pooling\n",
    "net2_mapped = Mapping([net2, mappings_1st_pool_2nd_pool], ds.max_dim_mappings[1,1], ds.max_dim_mappings[1,0], min_value = 0)\n",
    "net2_mapped = LL.DropoutLayer(net2_mapped, p=0.5)\n",
    "\n",
    "# convolution layer\n",
    "net3 = ApplyDescLayer([memb_2nd_pool, net2_mapped]) # patch operator layer\n",
    "net3 = LL.DenseLayer(net3, n_fout3, nonlinearity=L.nonlinearities.rectify)\n",
    "net3 = LL.ReshapeLayer(net3, (-1, ds.max_dim_mappings[1,0], n_fout3))\n",
    "\n",
    "# pooling\n",
    "net3_mapped = Reduce_to_centroids([net3, idx_centroids_conv3])\n",
    "net3_mapped = LL.DropoutLayer(net3_mapped, p=0.5)\n",
    "\n",
    "# classification layer\n",
    "cla = LL.DenseLayer(net3_mapped, n_digits, nonlinearity=L.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solver definition\n",
    "\n",
    "target    = T.ivector('target')\n",
    "pred      = LL.get_output(cla, deterministic=False)\n",
    "pred_test = LL.get_output(cla, deterministic=True)\n",
    "\n",
    "cost_dataterm = LO.categorical_crossentropy(pred, target).mean()\n",
    "acc = LO.categorical_accuracy(pred_test, target).mean() # classification accuracy\n",
    "\n",
    "# regularization\n",
    "mu = 1e-4\n",
    "cost_reg = mu * LR.regularize_network_params(net3_mapped, LR.l2)\n",
    "\n",
    "# cost function\n",
    "cost = cost_dataterm + cost_reg\n",
    "\n",
    "# gradient definition\n",
    "params    = LL.get_all_params(cla)\n",
    "grad      = T.grad(cost, params)\n",
    "grad_norm = T.nlinalg.norm(T.concatenate([g.flatten() for g in grad]), 2)\n",
    "\n",
    "# update function\n",
    "updates = L.updates.adam(cost, params, learning_rate=1e-4)\n",
    "\n",
    "# train / test functions\n",
    "funcs = dict()\n",
    "\n",
    "funcs['train'] = theano.function([vals.input_var, \n",
    "                                  P_rho_orig_img.input_var, P_theta_orig_img.input_var,\n",
    "                                  P_rho_1st_pool.input_var, P_theta_1st_pool.input_var,\n",
    "                                  P_rho_2nd_pool.input_var, P_theta_2nd_pool.input_var,\n",
    "                                  mappings_orig_graph_1st_pool.input_var, mappings_1st_pool_2nd_pool.input_var,\n",
    "                                  idx_centroids_conv3.input_var, target],\n",
    "                                [cost, cost_dataterm, cost_reg, grad_norm, acc],  \n",
    "                                updates = updates, \n",
    "                                allow_input_downcast='True',\n",
    "                                on_unused_input = 'warn')\n",
    "\n",
    "funcs['pred'] = theano.function([vals.input_var, \n",
    "                                 P_rho_orig_img.input_var, P_theta_orig_img.input_var,\n",
    "                                 P_rho_1st_pool.input_var, P_theta_1st_pool.input_var,\n",
    "                                 P_rho_2nd_pool.input_var, P_theta_2nd_pool.input_var,\n",
    "                                 mappings_orig_graph_1st_pool.input_var, mappings_1st_pool_2nd_pool.input_var,\n",
    "                                 idx_centroids_conv3.input_var, target], \n",
    "                                [cost, cost_dataterm, cost_dataterm, cost_dataterm, acc, pred],\n",
    "                                no_default_updates=True,\n",
    "                                allow_input_downcast='True',\n",
    "                                on_unused_input='ignore')\n",
    "\n",
    "print params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(num_samples, dim_batch, get_new_sample):\n",
    "    max_dim_laplacians = np.asarray([[ds.max_dim_mappings[0,1], ds.max_dim_mappings[0,1]],\n",
    "                                              [ds.max_dim_mappings[0,0], ds.max_dim_mappings[0,0]],\n",
    "                                              [ds.max_dim_mappings[1,0], ds.max_dim_mappings[1,0]]])\n",
    "    \n",
    "    num_img      = 0\n",
    "    overall_acc  = 0\n",
    "    overall_cost = 0\n",
    "    for k in range(num_samples//dim_batch):\n",
    "        num_samples_current_batch = np.min([(k+1)*dim_batch, num_samples]) - k*dim_batch\n",
    "        \n",
    "        c_batch = np.zeros((num_samples_current_batch, ds.max_dim_mappings[0,1], 1))\n",
    "        targets_batch = np.zeros((num_samples_current_batch,))\n",
    "\n",
    "        P_rho_orig_img_batch   = np.zeros((num_samples_current_batch, ds.max_dim_mappings[0,1], ds.max_dim_mappings[0,1]))\n",
    "        P_theta_orig_img_batch = np.zeros((num_samples_current_batch, ds.max_dim_mappings[0,1], ds.max_dim_mappings[0,1]))\n",
    "        P_rho_1st_pool_batch   = np.zeros((num_samples_current_batch, ds.max_dim_mappings[0,0], ds.max_dim_mappings[0,0]))\n",
    "        P_theta_1st_pool_batch = np.zeros((num_samples_current_batch, ds.max_dim_mappings[0,0], ds.max_dim_mappings[0,0]))\n",
    "        P_rho_2nd_pool_batch   = np.zeros((num_samples_current_batch, ds.max_dim_mappings[1,0], ds.max_dim_mappings[1,0]))\n",
    "        P_theta_2nd_pool_batch = np.zeros((num_samples_current_batch, ds.max_dim_mappings[1,0], ds.max_dim_mappings[1,0]))\n",
    "\n",
    "        mappings_orig_graph_1st_pool_batch = np.zeros((num_samples_current_batch, ds.max_dim_mappings[0,0], ds.max_dim_mappings[0,1]))\n",
    "        mappings_1st_pool_2nd_pool_batch   = np.zeros((num_samples_current_batch, ds.max_dim_mappings[1,0], ds.max_dim_mappings[1,1]))\n",
    "        \n",
    "        idx_centroids = np.zeros((num_samples_current_batch,), dtype='int32')\n",
    "        \n",
    "        for kk in range(num_samples_current_batch):\n",
    "            x_ = get_new_sample(num_img)\n",
    "            \n",
    "            c_batch[kk, :, 0]   = np.append(x_[0], np.zeros(max_dim_laplacians[0,0] - x_[0].shape[0],), axis=0)\n",
    "            targets_batch[kk]   = x_[1]\n",
    "            \n",
    "            P_rho_orig_img_batch[kk, :, :]   = extend_feat(x_[2], max_dim_laplacians[0,:], value=np.nan)\n",
    "            P_theta_orig_img_batch[kk, :, :] = extend_feat(x_[3], max_dim_laplacians[0,:], value=np.nan)\n",
    "            P_rho_1st_pool_batch[kk, :, :]   = extend_feat(x_[4], max_dim_laplacians[1,:], value=np.nan, patch=1)\n",
    "            P_theta_1st_pool_batch[kk, :, :] = extend_feat(x_[5], max_dim_laplacians[1,:], value=np.nan, patch=2)\n",
    "\n",
    "            P_rho_2nd_pool_batch[kk, :, :]   = extend_feat(x_[6], max_dim_laplacians[2,:], value=np.nan, patch=1)\n",
    "            P_theta_2nd_pool_batch[kk, :, :] = extend_feat(x_[7], max_dim_laplacians[2,:], value=np.nan, patch=2)\n",
    "\n",
    "            mappings_orig_graph_1st_pool_batch[kk, :] = extend_feat(x_[8], ds.max_dim_mappings[0,:], value=0)\n",
    "            mappings_1st_pool_2nd_pool_batch[kk, :]   = extend_feat(x_[9], ds.max_dim_mappings[1,:], value=0)\n",
    "            \n",
    "            idx_centroids[kk] = x_[10]\n",
    "            \n",
    "            num_img += 1\n",
    "            \n",
    "        c_parameters= (c_batch, \n",
    "                       P_rho_orig_img_batch, P_theta_orig_img_batch,\n",
    "                       P_rho_1st_pool_batch, P_theta_1st_pool_batch,\n",
    "                       P_rho_2nd_pool_batch, P_theta_2nd_pool_batch,\n",
    "                       mappings_orig_graph_1st_pool_batch, mappings_1st_pool_2nd_pool_batch,\n",
    "                       idx_centroids, targets_batch)\n",
    "        \n",
    "        tmp        = funcs['pred'](*c_parameters)\n",
    "        overall_acc  += tmp[4]*num_samples_current_batch\n",
    "        overall_cost += tmp[0]*num_samples_current_batch\n",
    "    return [overall_acc/num_samples, overall_cost/num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(): #helper function used for testing the model during training\n",
    "    dim_batch        = 100\n",
    "    num_test_samples = 10000\n",
    "    \n",
    "    return compute_acc(num_test_samples, dim_batch, ds.get_test_sample)\n",
    "\n",
    "def val_model(): #helper function used for validating the model during training\n",
    "    dim_batch       = 100\n",
    "    num_val_samples = 5000\n",
    "    \n",
    "    return compute_acc(num_val_samples, dim_batch, ds.get_train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training:\n",
    "np.random.seed(42)\n",
    "\n",
    "dim_mini_batch      = 10\n",
    "num_training_iter   = 350000\n",
    "val_test_interval   = 5000\n",
    "decreasing_lr_iter  = 280000\n",
    "iter_train          = 0\n",
    "cost_train_avg      = []\n",
    "grad_norm_train_avg = []\n",
    "acc_train_avg       = []\n",
    "cost_test_avg       = []\n",
    "grad_norm_test_avg  = []\n",
    "acc_test_avg        = []\n",
    "cost_val_avg        = []\n",
    "acc_val_avg         = []\n",
    "iter_test           = []\n",
    "list_training_time  = list()\n",
    "\n",
    "max_dim_laplacians  = np.asarray([[ds.max_dim_mappings[0,1], ds.max_dim_mappings[0,1]],\n",
    "                                                [ds.max_dim_mappings[0,0], ds.max_dim_mappings[0,0]],\n",
    "                                                [ds.max_dim_mappings[1,0], ds.max_dim_mappings[1,0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training code\n",
    "for i in xrange(num_training_iter):\n",
    "    if (len(cost_train_avg) % val_test_interval) == 0:\n",
    "        if (len(cost_train_avg)>0):\n",
    "            print \"[TRN] epoch = %03i, cost = %3.2e, |grad| = %.2e, acc = %3.2e (%03.2fs)\" % \\\n",
    "            (len(cost_train_avg), cost_train_avg[-1], grad_norm_train_avg[-1], acc_train_avg[-1], time.time() - tic)\n",
    "\n",
    "        tic = time.time()\n",
    "        acc_val, cost_val = val_model()\n",
    "        cost_val_avg.append(cost_val)\n",
    "        acc_val_avg.append(acc_val)\n",
    "        print \"[VAL] epoch = %03i, cost = %3.2e,acc = %3.2e (%03.2fs)\" % \\\n",
    "            (len(cost_train_avg), cost_val_avg[-1], acc_val_avg[-1],  time.time() - tic)\n",
    "            \n",
    "        iter_test.append(len(cost_train_avg))\n",
    "        tic = time.time()\n",
    "        acc_test, cost_test = test_model()\n",
    "        cost_test_avg.append(cost_test)\n",
    "        acc_test_avg.append(acc_test)\n",
    "        print \"[TST] epoch = %03i, cost = %3.2e,acc = %3.2e (%03.2fs)\" % \\\n",
    "            (len(cost_train_avg), cost_test_avg[-1], acc_test_avg[-1],  time.time() - tic)\n",
    "        \n",
    "    tic = time.time()\n",
    "\n",
    "    vals_batch = np.zeros((dim_mini_batch, ds.max_dim_mappings[0,1], 1))\n",
    "    targets_batch = np.zeros((dim_mini_batch,))\n",
    "    \n",
    "    P_rho_train_orig_img_batch   = np.zeros((dim_mini_batch, ds.max_dim_mappings[0,1], ds.max_dim_mappings[0,1]))\n",
    "    P_theta_train_orig_img_batch = np.zeros((dim_mini_batch, ds.max_dim_mappings[0,1], ds.max_dim_mappings[0,1]))\n",
    "    P_rho_train_1st_pool_batch   = np.zeros((dim_mini_batch, ds.max_dim_mappings[0,0], ds.max_dim_mappings[0,0]))\n",
    "    P_theta_train_1st_pool_batch = np.zeros((dim_mini_batch, ds.max_dim_mappings[0,0], ds.max_dim_mappings[0,0]))\n",
    "    P_rho_train_2nd_pool_batch   = np.zeros((dim_mini_batch, ds.max_dim_mappings[1,0], ds.max_dim_mappings[1,0]))\n",
    "    P_theta_train_2nd_pool_batch = np.zeros((dim_mini_batch, ds.max_dim_mappings[1,0], ds.max_dim_mappings[1,0]))\n",
    "\n",
    "    mappings_orig_graph_1st_pool_batch = np.zeros((dim_mini_batch, ds.max_dim_mappings[0,0], ds.max_dim_mappings[0,1]))\n",
    "    mappings_1st_pool_2nd_pool_batch   = np.zeros((dim_mini_batch, ds.max_dim_mappings[1,0], ds.max_dim_mappings[1,1]))\n",
    "           \n",
    "    idx_centroids = np.zeros((dim_mini_batch,), dtype='int32')\n",
    "    \n",
    "    num_img = 0  \n",
    "    for x_ in ds.train_iter():\n",
    "        if (num_img>=dim_mini_batch):\n",
    "            break\n",
    "            \n",
    "        vals_batch[num_img, :, 0] = np.append(x_[0], np.zeros(ds.max_dim_mappings[0,1] - x_[0].shape[0],), axis=0)\n",
    "        targets_batch[num_img]    = x_[1]\n",
    "\n",
    "        P_rho_train_orig_img_batch[num_img, :, :]   = extend_feat(x_[2], max_dim_laplacians[0,:], value=np.nan)\n",
    "        P_theta_train_orig_img_batch[num_img, :, :] = extend_feat(x_[3], max_dim_laplacians[0,:], value=np.nan)\n",
    "        P_rho_train_1st_pool_batch[num_img, :, :]   = extend_feat(x_[4], max_dim_laplacians[1,:], value=np.nan, patch=1)\n",
    "        P_theta_train_1st_pool_batch[num_img, :, :] = extend_feat(x_[5], max_dim_laplacians[1,:], value=np.nan, patch=2)\n",
    "\n",
    "        P_rho_train_2nd_pool_batch[num_img, :, :]   = extend_feat(x_[6], max_dim_laplacians[2,:], value=np.nan, patch=1)\n",
    "        P_theta_train_2nd_pool_batch[num_img, :, :] = extend_feat(x_[7], max_dim_laplacians[2,:], value=np.nan, patch=2)\n",
    "\n",
    "        mappings_orig_graph_1st_pool_batch[num_img, :] = extend_feat(x_[8], ds.max_dim_mappings[0,:], value=0)\n",
    "        mappings_1st_pool_2nd_pool_batch[num_img, :]   = extend_feat(x_[9], ds.max_dim_mappings[1,:], value=0)\n",
    "            \n",
    "        idx_centroids[num_img] = x_[10]\n",
    "            \n",
    "        num_img += 1\n",
    "        \n",
    "    parameters_train = (vals_batch, \n",
    "                        P_rho_train_orig_img_batch, P_theta_train_orig_img_batch,\n",
    "                        P_rho_train_1st_pool_batch, P_theta_train_1st_pool_batch,\n",
    "                        P_rho_train_2nd_pool_batch, P_theta_train_2nd_pool_batch,\n",
    "                        mappings_orig_graph_1st_pool_batch, mappings_1st_pool_2nd_pool_batch,\n",
    "                        idx_centroids, targets_batch)\n",
    "    tmp = funcs['train'](*parameters_train)\n",
    "\n",
    "    cost_train      = tmp[0]\n",
    "    grad_norm_train = tmp[3]\n",
    "    acc_train       = tmp[4]\n",
    "     \n",
    "    cost_train_avg.append(cost_train)\n",
    "    grad_norm_train_avg.append(grad_norm_train)\n",
    "    acc_train_avg.append(acc_train)   \n",
    "    train_time = time.time() - tic\n",
    "    list_training_time.append(train_time)\n",
    "    \n",
    "    \n",
    "    if (i==decreasing_lr_iter):\n",
    "        updates = L.updates.adam(cost, params, learning_rate=1e-5)\n",
    "\n",
    "        funcs['train'] = theano.function([vals.input_var, \n",
    "                                  P_rho_orig_img.input_var, P_theta_orig_img.input_var,\n",
    "                                  P_rho_1st_pool.input_var, P_theta_1st_pool.input_var,\n",
    "                                  P_rho_2nd_pool.input_var, P_theta_2nd_pool.input_var,\n",
    "                                  mappings_orig_graph_1st_pool.input_var, mappings_1st_pool_2nd_pool.input_var,\n",
    "                                  idx_centroids_conv3.input_var, target],\n",
    "                                [cost, cost_dataterm, cost_reg, grad_norm, acc],  \n",
    "                                updates = updates, \n",
    "                                allow_input_downcast='True',\n",
    "                                on_unused_input = 'warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(val_test_interval*np.asarray(range(len(cost_val_avg))), cost_val_avg, 'g-')\n",
    "ax2.plot(val_test_interval*np.asarray(range(len(cost_test_avg))), cost_test_avg, 'b-')\n",
    "\n",
    "ax1.set_xlabel('Training iteration')\n",
    "ax1.set_ylabel('Validation loss', color='g')\n",
    "ax2.set_ylabel('Test loss', color='b')\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(val_test_interval*np.asarray(range(len(acc_val_avg))), np.asarray(acc_val_avg), 'g-')\n",
    "ax2.plot(val_test_interval*np.asarray(range(len(acc_test_avg))), np.asarray(acc_test_avg), 'b-')\n",
    "\n",
    "print 'Max accuracy on test set achieved: %f%%' % np.asarray(acc_test_avg)[np.asarray(cost_val_avg)==np.min(cost_val_avg)]\n",
    "\n",
    "ax1.set_xlabel('Training iteration')\n",
    "ax1.set_ylabel('Validation accuracy', color='g')\n",
    "ax2.set_ylabel('Test accuracy', color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
